#!/bin/bash
# @(#) starts infrakit and deploys the configuration
# @(#) if no argument is provided, the configuration is expected to be in same directory

INFRAKIT_HOME=/infrakit
INFRAKIT_IMAGE_VERSION=${INFRAKIT_IMAGE_VERSION:-latest}
INFRAKIT_INFRAKIT_IMAGE=appcelerator/infrakit:0.4.0
AWS_FILES=(bootstrap.yml)
SSL_KEY_LENGTH=2048
BOOTSTRAP_VOLUME=amp-bootstrap
ROLE_LABEL=tools
INFRAKIT_OPTIONS="-e INFRAKIT_HOME=$INFRAKIT_HOME -v $BOOTSTRAP_VOLUME:$INFRAKIT_HOME --label=io.amp.role=$ROLE_LABEL"
INFRAKIT_PLUGINS_OPTIONS="-v /var/run/docker.sock:/var/run/docker.sock -e INFRAKIT_PLUGINS_DIR=$INFRAKIT_HOME/plugins"
BRIDGE_NETWORK=hostnet
CLUSTER_LABEL_NAME=${CLUSTER_LABEL_NAME:-atomiq.clusterid}
INFRAKIT_LOG_LEVEL=4
#WORKER_GROUP_LIST=(data:2)
WORKER_GROUP_LIST=()

# check if the bootstrap script is executed inside a container
_am_i_in_a_container() {
  if [ ! -d /proc ]; then
    return 1
  fi
  awk -F/ '$2 == "docker"' /proc/self/cgroup | read
}

# pull docker images
_pull_images() {
  local _images="infrakit $*"
  local _image
  local i
  for i in $_images; do
    _image=$(eval echo \$INFRAKIT_$(echo $i | tr '[:lower:]' '[:upper:]')_IMAGE)
    if [ -z "$_image" ]; then
      continue
    fi
    if ! docker pull "$_image" >&2; then
      # fall back to locally generated image
      if ! docker image ls "$_image" > /dev/null 2>&1; then
        echo "no image with name $_image" >&2
        exit 1
      fi
    fi
  done
}

_get_dirname() {
    local _d=$1
    if [ -z "$_d" ]; then
      echo "_get_dirname() expects an argument" >&2
      exit 1
    fi
    which realpath >/dev/null 2>&1
    if [ $? -eq 0 ]; then
      _d=$(realpath $_d)
    fi
    if [ ! -d "$_d" ]; then
      _d=$(dirname $_d)
    fi
    echo "$_d"
}

# expose the Docker remote api
# and set the registry mirrors
# only applies to non local deployments
_expose_remote_api() {
  local _registry_opt
  mkdir -p /etc/systemd/system/docker.service.d
  if [ $# -gt 0 ]; then
    _registry_opt="--registry-mirror=http://$1:5000"
    if [ "$1" != "127.0.0.1" ]; then
      _registry_opt="$_registry_opt --insecure-registry=http://$1:5000"
    fi
  fi
  cat > /etc/systemd/system/docker.service.d/docker.conf <<EOF
[Service]
ExecStart=
ExecStart=/usr/bin/dockerd -H fd:// -H 0.0.0.0:2375 -H unix:///var/run/docker.sock --experimental $_registry_opt
EOF
  # Restart Docker to let port listening take effect.
  systemctl daemon-reload
  systemctl restart docker.service
}

# init a Swarm cluster
# to be used when bootstrapping a full cluster with InfraKit, Infrakit running on the Swarm manager
# if an argument is provided, it's the ip of an existing manager that should be joined
_init_swarm() {
  local _token
  local _manager
  local _loop=0
  local _maxloop=120
  echo "initializing Docker" >&2
  wget -qO- https://get.docker.com/ | sh && \
  usermod -G docker ubuntu && \
  systemctl enable docker.service && \
  systemctl start docker.service && \
  if [ -n "$1" ]; then
    _manager=$1
    _expose_remote_api "$_manager"
    echo "joining a Docker Swarm..." >&2
    while [ $((++_loop)) -lt $_maxloop ]; do
      _token=$(docker -H "$_manager:2375" swarm join-token -q manager 2>/dev/null)
      if [ $? -eq 0 ] && [ -n "$_token" ]; then
        break
      fi
      sleep 1
    done
    if [ $_loop -lt $_maxloop ]; then
      _loop=0
      _maxloop=10
      while [ $((++_loop)) -lt $_maxloop ]; do
        sleep 1
        docker swarm join --token "$_token" "$_manager:2377"
        if [ $? -eq 0 ]; then
          break
        fi
      done
      if [ $_loop -ge $_maxloop ]; then
        echo "failed to join" >&2
        return 1
      fi
    else
      echo "failed to get the join token" >&2
      return 1
    fi
  else
    echo "initializing Docker Swarm" >&2
    _expose_remote_api 127.0.0.1
    docker swarm init
  fi
}


# prepare a Docker volume for the infrakit configuration
# works even when the Docker host is not local (running from inside a container on the host)
# args:
#   - path of the directory containing the configuration
_prepare_configuration_volume() {
      local _d=$1
      local _dockerfile
      local _dockerimage
      _dockerfile=$(mktemp $_d/ikt.vol.XXXXXX)
      cleanup_file_list="$cleanup_file_list $_dockerfile"
      _dockerimage=$(basename "$_dockerfile" | tr '[:upper:]' '[:lower:]')

      # in case a configuration file has been provided for the instance plugin
      if [ -n "$providerfile" ]; then
        mkdir -p "$_d/$provider"
        cp "$providerfile" "$_d/$provider/"
      fi

      echo "preparing a Docker image with the configuration data" >&2
      cat >> $_dockerfile << EOF
FROM alpine:3.5
WORKDIR $INFRAKIT_HOME
COPY . $INFRAKIT_HOME
EOF
      if [ -n "$envfile" ]; then
        local _envfile=$(mktemp $_d/ikt.env.XXXXXX)
        cleanup_file_list="$cleanup_file_list $_envfile"
        cp "$envfile" "$_envfile"
        cat >> $_dockerfile << EOF
COPY $(basename $_envfile) $INFRAKIT_HOME/env.ikt
EOF
      fi
      cat >> $_dockerfile << EOF
RUN mkdir -p $INFRAKIT_HOME/logs $INFRAKIT_HOME/plugins $INFRAKIT_HOME/configs
VOLUME $INFRAKIT_HOME
EOF
      cleanup_image_list="$cleanup_image_list $_dockerimage"
      docker build -t "$_dockerimage" -f "$_dockerfile" "$_d" >/dev/null|| exit 1
      docker volume inspect $BOOTSTRAP_VOLUME >/dev/null 2>&1
      if [ $? -eq 0 ]; then
        if [ $force_start -eq 1 ]; then
          echo "warning: force start, removing Docker volume $BOOTSTRAP_VOLUME" >&2
          docker volume rm $BOOTSTRAP_VOLUME >&2
          if [ $? -ne 0 ]; then
            echo "force start failed, unable to remove the Docker volume $BOOTSTRAP_VOLUME, it's probably still used by an infrakit container" >&2
            exit 1
          fi
        else
          echo "the Docker volume $BOOTSTRAP_VOLUME already exists, a cluster is probably already running, abort. You may want to use the -f option to force start." >&2
          exit 1
        fi
      fi
      echo "creating Docker volume $BOOTSTRAP_VOLUME with the configuration data" >&2
      docker run --rm -v $BOOTSTRAP_VOLUME:$INFRAKIT_HOME "$_dockerimage" true >&2
}

_system_check() {
  local mmcmin=262144
  typeset -i mmc
  # only check elasticsearch prerequisites on systems where it makes sense (Linux system)
  mmc=$(sysctl -n vm.max_map_count 2>/dev/null)
  if [[ $? -eq 0 ]]; then
    if [[ $mmc -lt $mmcmin ]]; then
      echo "your max map count limit is too low for elasticsearch" >&2
      echo "you should edit your /etc/sysctl.conf file and add the line 'vm.max_map_count = 262144'" >&2
      echo "or add a /etc/sysctl.d/99-amp.conf with that single line" >&2
      echo "to set it in your current session, you can run 'sudo sysctl -w vm.max_map_count=262144'" >&2
      return 1
    fi
  fi

  # if swarm is initialized, the modules will already be loaded
  docker info | grep -q "Swarm: active" && return 0
  # on Linux system, we have to make sure that the modules are loaded
  if [[ $(uname) = "Linux" ]]; then
    # grep for ip_vs_rr (if it's present then so is ip_vs)
    lsmod | grep -q ip_vs_rr
    if [[ $? -ne 0 ]]; then
      echo "Warning: required IPVS modules are not loaded. See: https://github.com/appcelerator/amp/tree/master/docs#linux"
      return 1
    fi
  fi
  return 0
}

# define and prepare the source directory
_set_source() {
  local _d
  # Location of InfraKit templates
  InfraKitConfigurationBaseURL=$1
  # if a remote location is provided, the configuration will be searched there
  if $(echo "$InfraKitConfigurationBaseURL" | grep -q "://"); then
    CONTAINER_CONFIG_TPL=$InfraKitConfigurationBaseURL/config.${provider}-${target}.tpl
    CONTAINER_PLUGINS_CFG=$InfraKitConfigurationBaseURL/plugins.json
    # a new random temporary directory used as the source
    _d=$(mktemp -d)
    if [ "x$provider" = "xaws" ]; then
      echo "trying to fetch remote files for $provider ($target)..." >&2
      mkdir -p "$_d/$provider"
      for i in "${AWS_FILES[@]}"; do
        curl -sL $InfraKitConfigurationBaseURL/${provider}-$target/$i -o $_d/$provider/$i && echo "  fetched $InfraKitConfigurationBaseURL/${provider}-$target/$i" >&2
      done
      echo "fetch done" >&2
    fi
  else
    CONTAINER_CONFIG_TPL=file://$INFRAKIT_HOME/config.${provider}-${target}.tpl
    CONTAINER_PLUGINS_CFG=file://$INFRAKIT_HOME/plugins.json
    # the provided path, or just use the local directory as the source
    _d=${InfraKitConfigurationBaseURL:-$0}
    which realpath >/dev/null 2>&1
    if [ $? -eq 0 ]; then
      _d=$(realpath $_d)
    fi
    if [ ! -d $_d ]; then
      _d=$(dirname $_d)
    fi
  fi
  _prepare_configuration_volume $_d
}

# sets the number of managers and workers
_set_size() {
  local _vars
  local _cattle_count
  local _s
  local _t
  local _w
  if [ "x$provider" = "xdocker" ]; then
    # we're adding a container to proxy the ports, so let's remove it from the overall count
    ((--worker_count))
  fi
  if [ $worker_count -le 0 ] || [ $manager_count -le 0 ]; then
    echo "asking for $manager_count manager(s) and $worker_count worker(s), abort" >&2
    exit 1
  fi
  _vars="{{ var \"/swarm/size/manager\" \"$manager_count\" }} {{ var \"/swarm/size/worker\" \"$worker_count\" }}"
  _cattle_count=$worker_count
  for _w in ${WORKER_GROUP_LIST[@]}; do
    _t=$(echo $_w | cut -d: -f1)
    _s=$(echo $_w | cut -d: -f2)
    _vars="$_vars {{ var \"/swarm/size/worker/$_t\" \"$_s\" }}"
    _cattle_count=$((_cattle_count - _s))
  done
  _vars="$_vars {{ var \"/swarm/size/worker/cattle\" \"$_cattle_count\" }}"
  docker run --rm -v $BOOTSTRAP_VOLUME:$INFRAKIT_HOME alpine:3.5 sh -c "echo '$_vars' >> $INFRAKIT_HOME/env.ikt" >&2 || exit 1
  echo "cluster size added to $INFRAKIT_HOME/env.ikt" >&2
}

# return a cluster id
_set_clusterid(){
  local _cid
  if [ -n "$label" ]; then
    _cid=$label
  else
    _cid=$(uuidgen 2>/dev/null | tr '[:upper:]' '[:lower:]')
    if [ ${PIPESTATUS[0]} -ne 0 ] || [ -z "$_cid" ]; then
      # fall back to the hostname if uuidgen is not available, probably the case in a container
      _cid=$(hostname)
    fi
  fi
  echo $_cid
}

# get the private IP
# first with the AWS metadata service, and then a more standard way
_get_ip() {
  local _ip
  _ip=$(curl -m 3 -f 169.254.169.254/latest/meta-data/local-ipv4 2>/dev/null) || _ip=$(ip a show dev eth0 2>/dev/null | grep inet | grep eth0 | tail -1 | sed -e 's/^.*inet.//g' -e 's/\/.*$//g')
  if [ -z "$_ip" ];then
    _ip=$(ifconfig $(netstat -nr | awk 'NF==6 && $1 ~/default/ {print $6}' | tail -1) | awk '$1 == "inet" {print $2}' | grep -v "127.0.0.1" | sed -e 's/addr://')
  fi
  if [ -z "$_ip" ];then
    echo "unable to guess the private IP" >&2
    exit 1
  fi
  echo $_ip
}

# the aws instance plugin has subtypes
_instance_plugin_compute_subtype(){
  case "$1" in
  aws)
    echo "ec2-instance"
    ;;
  esac
}

# run the infrakit containers
# return 1 if a new container has been started
_run_ikt_container() {
  local _should_wait_for_plugins=0
  local _infrakit_image
  docker run --rm -v $BOOTSTRAP_VOLUME:$INFRAKIT_HOME alpine:3.5 sh -c "echo 'group' >> $INFRAKIT_HOME/leader" || exit 1

  # remove old customized configuration just in case
  docker run --rm -v $BOOTSTRAP_VOLUME:$INFRAKIT_HOME alpine:3.5 sh -c "sed -i.bak 's/^{{ var \"\/script\/baseurl\".\*$//' $INFRAKIT_HOME/env.ikt" || exit 1
  local _extra_plugins=""
  # managed plugins should be started in the main infrakit container
  echo "$MANAGED_PLUGINS" | grep -qw $provider
  if [ $? -eq 0 ]; then
    _extra_plugins=instance-$provider
  fi
  docker container ls --format '{{.Names}}' | grep -qw infrakit
  if [ $? -ne 0 ]; then
    # cleanup
    docker run --rm -v $BOOTSTRAP_VOLUME:$INFRAKIT_HOME alpine:3.5 sh -c "rm -f $INFRAKIT_HOME/plugins/flavor-* $INFRAKIT_HOME/plugins/group* $INFRAKIT_HOME/plugins/instance-*" || exit 1
    # set a stackname if it's not already set. Useful when deploying on AWS
    docker run --rm -v $BOOTSTRAP_VOLUME:$INFRAKIT_HOME alpine:3.5 sh -c "grep -q /aws/stackname $INFRAKIT_HOME/env.ikt"
    if [ $? -ne 0 ]; then
      docker run --rm -v $BOOTSTRAP_VOLUME:$INFRAKIT_HOME alpine:3.5 sh -c "echo '{{ var \"/aws/stackname\" \"$clusterid\" }}' >> $INFRAKIT_HOME/env.ikt" || exit 1
    fi
    # set a docker engine label if it's not already set. Useful when deploying locally
    docker run --rm -v $BOOTSTRAP_VOLUME:$INFRAKIT_HOME alpine:3.5 sh -c "grep -q /docker/label/cluster/value $INFRAKIT_HOME/env.ikt"
    if [ $? -ne 0 ]; then
      docker run --rm -v $BOOTSTRAP_VOLUME:$INFRAKIT_HOME alpine:3.5 sh -c "echo '{{ var \"/docker/label/cluster/value\" \"$clusterid\" }}' >> $INFRAKIT_HOME/env.ikt" || exit 1
      echo "$clusterid"
    fi
    # set the provisioner IP if not already set. Useful when bootstraping with the first manager
    docker run --rm -v $BOOTSTRAP_VOLUME:$INFRAKIT_HOME alpine:3.5 sh -c "grep -q /bootstrap/ip $INFRAKIT_HOME/env.ikt"
    if [ $? -ne 0 ] && [ ! $(_am_i_in_a_container) ]; then
      local _ip
      _ip=$(_get_ip)
      docker run --rm -v $BOOTSTRAP_VOLUME:$INFRAKIT_HOME alpine:3.5 sh -c "echo '{{ var \"/bootstrap/ip\" \"$_ip\" }}' >> $INFRAKIT_HOME/env.ikt" || exit 1
      echo "bootstrap ip ($_ip) added in env.ikt" >&2 
    fi
    if [ "x$provider" = "xdocker" ]; then
        echo "making sure the network $BRIDGE_NETWORK is created" >&2
        docker network create -d bridge $BRIDGE_NETWORK  2>/dev/null
        INFRAKIT_OPTIONS="$INFRAKIT_OPTIONS --network $BRIDGE_NETWORK"
    fi

    _infrakit_image=$INFRAKIT_INFRAKIT_IMAGE

    echo "starting up InfraKit (image $_infrakit_image)" >&2
    docker run -d --restart always --name infrakit \
         $INFRAKIT_OPTIONS $INFRAKIT_PLUGINS_OPTIONS -e PLUGIN_DIR=$INFRAKIT_HOME/${provider} -e INFRAKIT_LOG_LEVEL=$INFRAKIT_LOG_LEVEL $_infrakit_image \
         infrakit plugin start --wait --config-url $CONTAINER_PLUGINS_CFG --exec os --log $INFRAKIT_LOG_LEVEL \
         manager group-stateless flavor-swarm flavor-vanilla flavor-combo $_extra_plugins >&2
    _should_wait_for_plugins=1
  else
    echo "infraKit container is already started" >&2
  fi
  return $_should_wait_for_plugins
}

# run an infrakit plugin as a container
# return 1 if a new plugin has been started
_run_ikt_plugin_container() {
  local _should_wait_for_plugins=0
  local _plugin
  local _image
  for _plugin in $@; do
    echo $MANAGED_PLUGINS | grep -qw $_plugin
    if [ $? -eq 0 ]; then
      # already managed by the main infrakit, no need to run it separately
      continue
    fi
    echo "$NON_CONTAINERIZED_PLUGINS" | grep -qw $_plugin
    if [ $? -ne 0 ]; then
      docker container ls --format '{{.Names}}' | grep -qw instance-plugin-$_plugin
      if [ $? -ne 0 ]; then
        # first, cleanup the pid and socket files
        # TODO: container way
        rm -f $INFRAKIT_HOME/plugins/instance-${_plugin}*
        _image=$(eval echo \${INFRAKIT_$(echo $_plugin | tr '[:lower:]' '[:upper:]')_IMAGE})
        if [ -z "$_image" ]; then
            echo "no image defined for plugin $_plugin" >&2
            exit 1
        fi
        if [ "$_plugin" = "docker" ]; then
            INFRAKIT_OPTIONS="$INFRAKIT_OPTIONS --network $BRIDGE_NETWORK"
        fi
        echo "starting up InfraKit $_plugin plugin (image $_image)..." >&2
        docker run -d --restart always --name instance-plugin-$_plugin \
             $INFRAKIT_OPTIONS $INFRAKIT_PLUGINS_OPTIONS $_image \
             infrakit-instance-$_plugin --log $INFRAKIT_LOG_LEVEL >&2
        if [ $? -ne 0 ]; then
            echo "unable to start the $_plugin plugin" >&2
            exit 1
        fi
        _should_wait_for_plugins=1
      else
        echo "$_plugin container is already running" >&2
      fi
    fi
  done
  return $_should_wait_for_plugins
}

# destroy the instances managed by infrakit
_destroy_groups() {
  local _groups
  local _group
  _groups=$(docker exec infrakit infrakit group ls 2>/dev/null | tail -n +2)
  for _group in $_groups; do
    docker exec infrakit grep -q "\"$_group\"" $INFRAKIT_HOME/config.json
    if [ $? -eq 0 ]; then
      docker exec infrakit infrakit group destroy $_group
    fi
  done
}

# kill the infrakit container
_kill_ikt() {
  docker container rm -f infrakit >/dev/null 2>&1 || killall infrakit >/dev/null 2>&1 && echo "infrakit has been destroyed" >&2
}

# kill the registry container (but keep the volume)
_kill_registry(){
  docker container rm -f registry >/dev/null 2>&1 && echo "registry has been destroyed" >&2
  docker container rm -f registry-cache >/dev/null 2>&1 && echo "registry-cache has been destroyed" >&2
}

# kill the infrakit plugins (container or process)
_kill_plugins() {
  local _plugin
  for _plugin in $VALID_PROVIDERS; do
    docker container rm -f instance-plugin-$_plugin >/dev/null 2>&1 || killall infrakit-instance-$_plugin >/dev/null 2>&1 && echo "$_plugin plugin has been destroyed" >&2
  done
}

# removes the configuration files
_clean_config() {
  docker volume inspect $BOOTSTRAP_VOLUME >/dev/null 2>&1
  if [ $? -eq 0 ]; then
    docker volume rm $BOOTSTRAP_VOLUME >/dev/null 2>&1 && echo "configuration volume deleted" >&2
  else
    echo "no configuration volume to delete" >&2
  fi
}

# convert the template of the configuration file
_prepare_config_container() {
  echo "prepare the InfraKit configuration file..." >&2
  local _config=$(mktemp)
  cleanup_file_list="$cleanup_file_list $_config"
  docker exec infrakit infrakit template --log $INFRAKIT_LOG_LEVEL $CONTAINER_CONFIG_TPL > $_config
  if [ $? -ne 0 ]; then
    echo "failed, template URL was $CONTAINER_CONFIG_TPL" >&2
    exit 1
  fi
  docker cp $_config infrakit:$INFRAKIT_HOME/config.json || exit 1
}

# deploy the infrakit configuration
_deploy_config_container() {
  echo "deploy the configuration..." >&2
  docker exec infrakit infrakit manager commit file://$INFRAKIT_HOME/config.json >&2
}

# run registry services for Docker image availability in the cluster
_run_registry() {
  local _nwopt
  docker network inspect $BRIDGE_NETWORK >/dev/null 2>&1 && _nwopt="--network=$BRIDGE_NETWORK"
  docker ps | grep -qw registry || \
  docker run --name=registry --detach $_nwopt \
             -p=5000:5000 --restart=unless-stopped \
             --volume=registry:/var/lib/registry \
             --label="$CLUSTER_LABEL_NAME=$clusterid" --label="io.amp.role=$ROLE_LABEL" \
             -e REGISTRY_STORAGE_DELETE_ENABLED=true \
             registry:2 >&2
  # for local deployment a registry cache is also created to boost the pulls
  [[ "$target" = "local" ]] && \
  docker run --name=registry-cache --detach $_nwopt \
             --restart=unless-stopped \
             --volume=registry-cache:/var/lib/registry \
             --label="$CLUSTER_LABEL_NAME=$clusterid" --label="io.amp.role=$ROLE_LABEL" \
             -e REGISTRY_STORAGE_DELETE_ENABLED=true \
             -e REGISTRY_PROXY_REMOTEURL="https://registry-1.docker.io" \
             registry:2 >&2
}

_status(){
  local _clusterid=$1
  local _groups
  local _group
  local _esize
  local _csize
  local _rc=0
  local _c
  _groups=$(docker exec infrakit infrakit group ls -q | grep $_clusterid)
  if [ $? -ne 0 ]; then
    echo "no InfraKit group definition found for this cluster" >&2
    return 1
  fi
  for _group in $_groups; do
    # first look for convergence in the group metadata, that will make sure the expected size is defined
    _c=$(docker exec infrakit infrakit metadata cat group-stateless/groups/$_group/Converged)
    if [ "x$_c" != "xtrue" ]; then
      echo "group specifications are not ready yet" >&2
      return 1
    fi
    # expected size
    _esize=$(docker exec infrakit infrakit metadata cat group-stateless/specs/$_group/Properties/Allocation/Size)
    if [ $? -ne 0 ]; then
      echo "failed to get metadata for group $_group" >&2
      return 1
    fi
    if [ $_esize -eq 0 ]; then
      _esize=$(docker exec infrakit infrakit metadata cat group-stateless/specs/$_group/Properties/Allocation/LogicalIDs | awk -F ',' '{print NF}')
      if [ $? -ne 0 ]; then
        echo "failed to get metadata for group $_group" >&2
        return 1
      fi
    fi
    # current size
    _csize=$(docker exec infrakit infrakit metadata ls group-stateless/groups/$_group/Instances | wc -l)
    if [ $? -ne 0 ]; then
      echo "failed to get metadata for group $_group" >&2
      return 1
    fi
    if [ $_csize -ne $_esize ]; then
      echo "$_group has not converged: expected size = $_esize, current size: $_csize" >&2
      ((++_rc))
    else
      echo "$_group has converged: size = $_esize" >&2
    fi
  done
  if [ $_rc -eq 0 ]; then
    echo "all groups have converged" >&2
  fi
  return $_rc
}

_list(){
  local _clusterid="$1"
  local _groups
  local _group
  _groups=$(docker exec infrakit infrakit group ls -q | grep "$_clusterid")
  if [ $? -ne 0 ]; then
    echo "no InfraKit group definition found for this cluster" >&2
    return 1
  fi
  for _group in $_groups; do
    docker exec infrakit infrakit instance --name="instance-$provider${subtype:+/$subtype}" describe --tags atomiq.clusterid="$_clusterid" --tags infrakit.group="$_group" -q 2>&1 | awk -v clusterid="$_clusterid" -v group="$_group" '{print clusterid, group, $2, $1}'
    if [ $? -ne 0 ]; then
      echo "failed to describe group $_group" >&2
      return 1
    fi
  done
}

_finish(){
  local _f
  for _f in $cleanup_file_list; do
    echo "cleaning up file $_f "
    rm -f "$_f"
  done
  for _i in $cleanup_image_list; do
    echo "cleaning up image $_i "
    docker image rm  "$_i" > /dev/null
  done
}

# where to deploy
VALID_TARGETS="aws local"
# Infrakit plugins used for deployment
VALID_PROVIDERS="aws docker"
# we can't run InfraKit in a container for these plugins
NON_CONTAINERIZED_PLUGINS=""
# providers managed by infrakit (integrated plugin)
MANAGED_PLUGINS="docker aws"
target=local
provider=docker
default_provider=1
envfile=""
providerfile=""
manager_count=1
worker_count=2
clean=0
init_swarm=0
swarm_join_ip=""
label=""
force_start=0
status_request=""
list_request=""
block=0
cleanup_file_list=""
cleanup_image_list=""
while getopts ":1j:t:p:e:m:w:i:l:s:hfdb:" opt; do
  case $opt in
  1)
      init_swarm=1
      ;;
  j)
      swarm_join_ip=$OPTARG
      ;;
  w)
      worker_count=$OPTARG
      ;;
  m)
      manager_count=$OPTARG
      ;;
  i)
      # clusterid
      label=$OPTARG
      ;;
  t)
      echo "$VALID_TARGETS" | grep -wq "$OPTARG" && target=$OPTARG
      if [ -z "$target" ]; then
          echo "valid targets are $VALID_TARGETS" >&2
          exit 1
      fi
      ;;
  p)
      # provider[:envfilepath]]
      f1=$(echo "$OPTARG" | cut -f1 -d:)
      f2=$(echo "$OPTARG" | cut -f2 -d:)
      if [ -n "$f2" ] && [ "x$f1" != "x$f2" ]; then
        providerfile="$f2"
        if [ ! -f "$providerfile" ]; then
          echo "Configuration file for $f1 was not found ($providerfile)" >&2
          exit 1
        fi
      fi
      echo "$VALID_PROVIDERS" | grep -wq "$f1" && provider=$f1
      if [ -z "$provider" ]; then
          echo "valid providers are $VALID_PROVIDERS" >&2
          exit 1
      fi
      default_provider=0
      ;;
  l)
      list_request=$OPTARG
      ;;
  s)
      status_request=$OPTARG
      ;;
  b)
      block=$OPTARG
      ;;
  h)
      echo "usage: $(basename $0) [-t target] [-p provider] [-m manager_count] [-w worker_count] [-i CLUSTERID] [-b TIMEOUT] [-l CLUSTERID] [-s CLUSTERID] [--init-swarm] [--join-swarm] [-f] [-h]"
      exit 0
      ;;
  f)
      # force start
      force_start=1
      ;;
  d)
      clean=1
      ;;
  e)
      envfile=$OPTARG
      if [ ! -f "$envfile" ]; then
        echo "$envfile does not exist" >&2
        exit 1
      fi
      ;;
  \?)
      echo "invalid option: -$OPTARG" >&2
      exit 1
      ;;
  :)
      echo "option -$OPTARG requires an argument." >&2
      exit 1
      ;;
  esac
done
shift "$((OPTIND-1))"

trap _finish EXIT

# in case only the target has been set, and it's not local, use the native plugin as default provider
if [ "x$target" != "xlocal" ] && [ $default_provider -eq 1 ]; then
    provider="$target"
fi
subtype=$(_instance_plugin_compute_subtype $provider)

if [ -n "$status_request" ]; then
  _status "$status_request"
  exit $?
fi
if [ -n "$list_request" ]; then
  _list "$list_request"
  exit $?
fi
if [ $clean -eq 1 ]; then
  _destroy_groups
  _kill_plugins
  _kill_ikt
  _kill_registry
  _clean_config
  exit
fi
if [ $init_swarm -eq 1 ]; then
  _init_swarm || exit 1
fi
if [ -n "$swarm_join_ip" ]; then
  _init_swarm "$swarm_join_ip" || exit 1
  exit 0
fi
[[ $target = "local" ]] && ( _system_check || exit 1 )
_set_source "$1"
_set_size
clusterid=$(_set_clusterid)
if [ "$provider" = "docker" ] || [ $init_swarm -eq 1 ]; then docker network create -d bridge "$BRIDGE_NETWORK"  2>/dev/null; _run_registry; fi
_pull_images "$provider"
_run_ikt_container "$provider"
started=$?
_run_ikt_plugin_container "$provider"
started=$((started + $?))
if [ $started -gt 0 ]; then
  # wait for infrakit and its plugins to be available, at least 3 consecutive times"
  echo -n "waiting for plugins to be available..." >&2
  rc=1
  loop=1
  looplimit=20
  ready=0
  while [ $ready -lt 3 ]; do
    docker exec infrakit infrakit manager inspect 2>&1 | grep -vq 'non-leader'
    rc=$?
    docker exec infrakit infrakit plugin ls 2>/dev/null | grep -q "instance-$provider"
    rc=$((rc+$?))
    docker exec infrakit infrakit instance --name="instance-$provider${subtype:+/$subtype}" describe >/dev/null 2>&1
    rc=$((rc+$?))
    if [ $((++loop)) -gt $looplimit ]; then
      echo " aborting after $looplimit attempts" >&2
      docker exec infrakit infrakit manager inspect >/dev/null
      docker exec infrakit infrakit plugin ls 2>/dev/null
      docker exec infrakit infrakit instance --name="instance-$provider${subtype:+/$subtype}" describe
      # docker inspect infrakit >&2
      docker logs infrakit >&2
      echo "#####################      manager logs        ############################" >&2
      docker exec infrakit cat /infrakit/logs/manager.log >&2
      echo "#####################        group logs        ############################" >&2
      docker exec infrakit cat /infrakit/logs/group-stateless.log >&2
      echo "#####################     instance logs        ############################" >&2
      docker exec infrakit cat "/infrakit/logs/instance-$provider.log" >&2
      echo "#####################       flavor logs        ############################" >&2
      docker exec infrakit cat /infrakit/logs/flavor-swarm.log >&2
      echo "#####################       configuration      ############################" >&2
      docker exec infrakit cat /infrakit/config.json >&2
      exit 1
    fi
    sleep 1
    echo -n "." >&2
    [[ $rc -eq 0 ]] && ((ready++))
  done
  echo >&2
fi
_prepare_config_container
_deploy_config_container
if [ $block -gt 0 ]; then
  rc=1
  SECONDS=0
  echo "waiting for cluster to be ready..." >&2
  while [ $rc -ne 0 ]; do
    _status $clusterid 2>/dev/null
    rc=$?
    if [ $SECONDS -gt $block ]; then
      echo "cluster is still not ready after $block sec" >&2
      exit 1
    fi
  done
fi
echo "done" >&2
