#!/bin/bash
# @(#) starts infrakit and deploys the configuration
# @(#) if no argument is provided, the configuration is expected to be in same directory

INFRAKIT_HOME=/infrakit
INFRAKIT_IMAGE_VERSION=${INFRAKIT_IMAGE_VERSION:-0.4.1}
#INFRAKIT_INFRAKIT_IMAGE=infrakit/devbundle:$INFRAKIT_IMAGE_VERSION
INFRAKIT_INFRAKIT_IMAGE=appcelerator/infrakit:latest
INFRAKIT_AWS_IMAGE=infrakit/aws:$INFRAKIT_IMAGE_VERSION
INFRAKIT_DOCKER_IMAGE=appcelerator/infrakit-docker:1.0.0
TERRAFORM_FILES=(cluster.tf variables.tf terraform.tfvars user-data)
SSL_KEY_LENGTH=2048
CERTIFICATE_SERVER_IMAGE=ndegory/certauth:latest
CERT_DIR=~/.config/infrakit/certs
BOOTSTRAP_VOLUME=amp-bootstrap
INFRAKIT_OPTIONS="-e INFRAKIT_HOME=$INFRAKIT_HOME -v $BOOTSTRAP_VOLUME:$INFRAKIT_HOME"
INFRAKIT_PLUGINS_OPTIONS="-v /var/run/docker.sock:/var/run/docker.sock -e INFRAKIT_PLUGINS_DIR=$INFRAKIT_HOME/plugins"
BRIDGE_NETWORK=hostnet
CLUSTER_LABEL_NAME=${CLUSTER_LABEL_NAME:-atomiq.clusterid}

# check if the bootstrap script is executed inside a container
_am_i_in_a_container() {
  if [ ! -d /proc ]; then
    return 1
  fi
  awk -F/ '$2 == "docker"' /proc/self/cgroup | read
}

# pull docker images
_pull_images() {
  local _images="infrakit $@"
  local _image
  local i
  for i in $_images; do
    _image=$(eval echo \$INFRAKIT_$(echo $i | tr '[:lower:]' '[:upper:]')_IMAGE)
    if [ -z "$_image" ]; then
      continue
    fi
    docker pull $_image >&2
    if [ $? -ne 0 ]; then
      # fall back to locally generated image
      docker image ls $_image > /dev/null 2>&1
      if [ $? -ne 0 ]; then
        echo "no image with name $_image" >&2
        exit 1
      fi
    fi
  done
}

_get_dirname() {
    local _d=$1
    if [ -z "$_d" ]; then
      echo "_get_dirname() expects an argument" >&2
      exit 1
    fi
    which realpath >/dev/null 2>&1
    if [ $? -eq 0 ]; then
      _d=$(realpath $_d)
    fi
    if [ ! -d $_d ]; then
      _d=$(dirname $_d)
    fi
    echo "$_d"
}

# expose the Docker remote api
_expose_remote_api() {
  mkdir -p /etc/systemd/system/docker.service.d
  cat > /etc/systemd/system/docker.service.d/docker.conf <<EOF
[Service]
ExecStart=
ExecStart=/usr/bin/dockerd -H fd:// -H 0.0.0.0:2375 -H unix:///var/run/docker.sock
EOF
  # Restart Docker to let port listening take effect.
  systemctl daemon-reload
  systemctl restart docker.service
}

# init a Swarm cluster
# to be used when bootstrapping a full cluster with InfraKit, Infrakit running on the Swarm manager
# if an argument is provided, it's the ip of an existing manager that should be joined
_init_swarm() {
  local _token
  local _manager
  local _loop=0
  local _maxloop=120
  echo "initializing Docker" >&2
  wget -qO- https://get.docker.com/ | sh && \
  usermod -G docker ubuntu && \
  systemctl enable docker.service && \
  systemctl start docker.service && \
  if [ -n "$1" ]; then
    _manager=$1
    echo "joining a Docker Swarm..." >&2
    while [ $((++_loop)) -lt $_maxloop ]; do
      _token=$(docker -H $_manager:2375 swarm join-token -q manager 2>/dev/null)
      if [ $? -eq 0 ] && [ -n "$_token" ]; then
        break
      fi
      sleep 1
    done
    if [ $_loop -lt $_maxloop ]; then
      _loop=0
      _maxloop=10
      while [ $((++_loop)) -lt $_maxloop ]; do
        sleep 1
        docker swarm join --token $_token $_manager:2377
        if [ $? -eq 0 ]; then
          break
        fi
      done
      if [ $_loop -ge $_maxloop ]; then
        echo "failed to join" >&2
        return 1
      fi
    else
      echo "failed to get the join token" >&2
      return 1
    fi
  else
    echo "initializing Docker Swarm" >&2
    _expose_remote_api
    docker swarm init
  fi
}

# build a new image based on the official one if needed
# TODO: remove the image when it's not needed anymore
_get_infrakit_image() {
  local _d
  local _package=$1
    if [ -z "$_package" ]; then
      echo "_get_infrakit_image() expects an argument" >&2
      exit 1
    fi
  echo "$MANAGED_PLUGINS" | grep -qw "$_package"
  if [ $? -ne 0 ]; then
    echo "$INFRAKIT_INFRAKIT_IMAGE"
    return
  fi
  _d=$(_get_dirname $0)
  local _dockerfile=$(mktemp $_d/tmp.XXXXXX)
  local _dockerimage=infrakit.$(basename $_dockerfile | tr '[:upper:]' '[:lower:]')
  echo "building a new infrakit image ($_dockerimage)" >&2
  cat >> $_dockerfile << EOF
FROM $INFRAKIT_INFRAKIT_IMAGE
RUN echo "@edge http://nl.alpinelinux.org/alpine/edge/main" >> /etc/apk/repositories && \
    echo "@testing http://nl.alpinelinux.org/alpine/edge/testing" >> /etc/apk/repositories && \
    echo "@community http://nl.alpinelinux.org/alpine/edge/community" >> /etc/apk/repositories
RUN apk add --update ${_package}@community && rm -rf /var/cache/apk/*
EOF
  docker build -f $_dockerfile -t $_dockerimage $_d >/dev/null
  if [ $? -eq 0 ]; then
    rm $_dockerfile
    echo $_dockerimage
    return 0
  else
    rm $_dockerfile
    return 1
  fi
}

# prepare a Docker volume for the infrakit configuration
# works even when the Docker host is not local (running from inside a container on the host)
# args:
#   - path of the directory containing the configuration
_prepare_configuration_volume() {
      local _d=$1
      local _dockerfile
      local _dockerimage
      _dockerfile=$(mktemp $_d/ikt.vol.XXXXXX)
      _dockerimage=$(basename $_dockerfile | tr '[:upper:]' '[:lower:]')

      # in case a configuration file has been provided for the instance plugin
      if [ -n "$providerfile" ]; then
        mkdir -p "$_d/$provider"
        cp "$providerfile" "$_d/$provider/"
      fi

      echo "preparing a Docker image with the configuration data" >&2
      cat >> $_dockerfile << EOF
FROM alpine:3.5
WORKDIR $INFRAKIT_HOME
COPY . $INFRAKIT_HOME
EOF
      if [ -n "$envfile" ]; then
        local _envfile=$(mktemp $_d/ikt.env.XXXXXX)
        cp "$envfile" $_envfile
        cat >> $_dockerfile << EOF
COPY $(basename $_envfile) $INFRAKIT_HOME/env.ikt
EOF
      fi
      cat >> $_dockerfile << EOF
RUN mkdir -p $INFRAKIT_HOME/logs $INFRAKIT_HOME/plugins $INFRAKIT_HOME/configs
VOLUME $INFRAKIT_HOME
EOF
      docker build -t $_dockerimage -f $_dockerfile $_d >/dev/null|| exit 1
      if [ -n "$envfile" ]; then rm "$_envfile"; fi
      docker volume inspect $BOOTSTRAP_VOLUME >/dev/null 2>&1
      if [ $? -eq 0 ]; then
        if [ $force_start -eq 1 ]; then
          echo "warning: force start, removing Docker volume $BOOTSTRAP_VOLUME" >&2
          docker volume rm $BOOTSTRAP_VOLUME
          if [ $? -ne 0 ]; then
            echo "force start failed, unable to remove the Docker volume $BOOTSTRAP_VOLUME, it's probably still used by an infrakit container" >&2
            exit 1
          fi
        else
          echo "the Docker volume $BOOTSTRAP_VOLUME already exists, a cluster is probably already running, abort. You may want to use the -f option to force start." >&2
          rm -f $_dockerfile
          exit 1
        fi
      fi
      echo "creating Docker volume $BOOTSTRAP_VOLUME with the configuration data" >&2
      docker run --rm -v $BOOTSTRAP_VOLUME:$INFRAKIT_HOME $_dockerimage true >&2
      rm -f $_dockerfile
      docker image rm $_dockerimage >/dev/null
}

# define and prepare the source directory
_set_source() {
  local _d
  # Location of InfraKit templates
  InfraKitConfigurationBaseURL=$1
  # if a remote location is provided, the configuration will be searched there
  if $(echo "$InfraKitConfigurationBaseURL" | grep -q "://"); then
    CONTAINER_CONFIG_TPL=$InfraKitConfigurationBaseURL/config.${provider}-${target}.tpl
    CONTAINER_PLUGINS_CFG=$InfraKitConfigurationBaseURL/plugins.json
    # a new random temporary directory used as the source
    _d=$(mktemp -d)
    if [ "x$provider" = "xterraform" ]; then
      echo "trying to fetch remote files for $provider ($target)..." >&2
      mkdir -p "$_d/$provider"
      for i in "${TERRAFORM_FILES[@]}"; do
        curl -sL $InfraKitConfigurationBaseURL/${provider}-$target/$i -o $_d/$provider/$i && echo "  fetched $InfraKitConfigurationBaseURL/${provider}-$target/$i" >&2
      done
      echo "fetch done" >&2
    fi
  else
    CONTAINER_CONFIG_TPL=file://$INFRAKIT_HOME/config.${provider}-${target}.tpl
    CONTAINER_PLUGINS_CFG=file://$INFRAKIT_HOME/plugins.json
    # the provided path, or just use the local directory as the source
    _d=${InfraKitConfigurationBaseURL:-$0}
    which realpath >/dev/null 2>&1
    if [ $? -eq 0 ]; then
      _d=$(realpath $_d)
    fi
    if [ ! -d $_d ]; then
      _d=$(dirname $_d)
    fi
  fi
  _prepare_configuration_volume $_d
}

# sets the number of managers and workers
_set_size() {
  local _vars
  if [ "x$provider" = "xdocker" ]; then
    # we're adding a container to proxy the ports, so let's remove it from the overall count
    ((--worker_count))
  fi
  if [ $worker_count -le 0 ] || [ $manager_count -le 0 ]; then
    echo "asking for $manager_count manager(s) and $worker_count worker(s), abort" >&2
    exit 1
  fi
  _vars="{{ global \"/swarm/size/manager\" \"$manager_count\" }}
{{ global \"/swarm/size/worker\" \"$worker_count\" }}"
  docker run --rm -v $BOOTSTRAP_VOLUME:$INFRAKIT_HOME alpine:3.5 sh -c "echo '$_vars' >> $INFRAKIT_HOME/env.ikt" >&2 || exit 1
  echo "cluster size added to $INFRAKIT_HOME/env.ikt" >&2
}

# return a cluster id
_set_clusterid(){
  local _cid
  if [ -n "$label" ]; then
    _cid=$label
  else
    _cid=$(uuidgen 2>/dev/null | tr '[:upper:]' '[:lower:]')
    if [ ${PIPESTATUS[0]} -ne 0 ] || [ -z "$_cid" ]; then
      # fall back to the hostname if uuidgen is not available, probably the case in a container
      _cid=$(hostname)
    fi
  fi
  echo $_cid
}

# get the private IP
# first with the AWS metadata service, and then a more standard way
_get_ip() {
  local _ip
  _ip=$(curl -m 3 -f 169.254.169.254/latest/meta-data/local-ipv4 2>/dev/null) || _ip=$(ip a show dev eth0 2>/dev/null | grep inet | grep eth0 | tail -1 | sed -e 's/^.*inet.//g' -e 's/\/.*$//g')
  if [ -z "$_ip" ];then
    _ip=$(ifconfig $(netstat -nr | awk 'NF==6 && $1 ~/default/ {print $6}' | tail -1) | awk '$1 == "inet" {print $2}' | grep -v "127.0.0.1" | sed -e 's/addr://')
  fi
  if [ -z "$_ip" ];then
    echo "unable to guess the private IP" >&2
    exit 1
  fi
  echo $_ip
}

# run a certificate signing service
_run_certificate_service() {
  local IP
  IP=$(_get_ip)
  docker container ls --format '{{.Names}}' | grep -qw certauth
  if [ $? -eq 0 ]; then
    echo "the certificate management service is already running" >&2
  else
    echo "warning: the certificate service is not production ready, use it only for development purposes" >&2
    cad=$(mktemp -d)
    if [ $? -ne 0 ] || [ -z "$cad" ]; then
      echo "unable to create a temporary directory to build the certificate management" >&2
      exit 1
    fi
    echo "build a certificate management service (in $cad)..." >&2
    pushd $cad
    git clone https://github.com/ndegory/certificate.authority.git
    pushd certificate.authority 2>/dev/null
    docker build -t $CERTIFICATE_SERVER_IMAGE . || exit 1
    popd -1 2>/dev/null && popd 2>/dev/null
    rm -rf $cad
    echo "temporary directory has been removed" >&2
    echo "run the certificate management service..." >&2
    docker run -d --restart always -p 80 --name certauth $CERTIFICATE_SERVER_IMAGE || exit 1
  fi
  CERTIFICATE_SERVER_PORT=$(docker inspect certauth --format='{{(index (index .NetworkSettings.Ports "80/tcp") 0).HostPort}}')
  echo "certificate server listening on port $CERTIFICATE_SERVER_PORT" >&2
  _vars="{{ global \"/certificate/ca/service\" \"$IP:$CERTIFICATE_SERVER_PORT\" }}"
  docker run --rm -v $BOOTSTRAP_VOLUME:$INFRAKIT_HOME alpine:3.5 sh -c "echo '$_vars' >> $INFRAKIT_HOME/env.ikt" || exit 1
}

# generate a certificate for the Docker client
_get_client_certificate() {
  mkdir -p $CERT_DIR
  if [ $? -ne 0 ]; then
    exit 1
  fi
  if [ ! -f $CERT_DIR/ca.pem ]; then
    echo "generating a self-signed CA..." >&2
    # (used by the Swarm flavor plugin)
    curl http://localhost:$CERTIFICATE_SERVER_PORT/ca > $CERT_DIR/ca.pem
    echo "generating a certificate for the Docker client in $CERT_DIR..." >&2
    openssl genrsa -out $CERT_DIR/client-key.pem $SSL_KEY_LENGTH
    openssl req -subj '/CN=client' -new -key $CERT_DIR/client-key.pem -out $CERT_DIR/client.csr
    curl --data "csr=$(cat $CERT_DIR/client.csr | sed 's/+/%2B/g');ext=extendedKeyUsage=clientAuth" http://localhost:$CERTIFICATE_SERVER_PORT/csr > $CERT_DIR/client.pem
    ls -l $CERT_DIR/client.pem
    rm -f $CERT_DIR/client.csr
  fi
}

# run the infrakit containers
# return 1 if a new container has been started
_run_ikt_container() {
  local _should_wait_for_plugins=0
  local _infrakit_image
  docker run --rm -v $BOOTSTRAP_VOLUME:$INFRAKIT_HOME alpine:3.5 sh -c "echo 'group' >> $INFRAKIT_HOME/leader" || exit 1

  # remove old customized configuration just in case
  docker run --rm -v $BOOTSTRAP_VOLUME:$INFRAKIT_HOME alpine:3.5 sh -c "sed -i.bak 's/^{{ global \"\/script\/baseurl\".\*$//' $INFRAKIT_HOME/env.ikt" || exit 1
  local _extra_plugins=""
  # managed plugins should be started in the main infrakit container
  echo "$MANAGED_PLUGINS" | grep -qw $provider
  if [ $? -eq 0 ]; then
    _extra_plugins=instance-$provider
  fi
  docker container ls --format '{{.Names}}' | grep -qw infrakit
  if [ $? -ne 0 ]; then
    # cleanup
    docker run --rm -v $BOOTSTRAP_VOLUME:$INFRAKIT_HOME alpine:3.5 sh -c "rm -f $INFRAKIT_HOME/plugins/flavor-* $INFRAKIT_HOME/plugins/group* $INFRAKIT_HOME/plugins/instance-*" || exit 1
    # set a stackname if it's not already set. Useful when deploying on AWS
    docker run --rm -v $BOOTSTRAP_VOLUME:$INFRAKIT_HOME alpine:3.5 sh -c "grep -q /aws/stackname $INFRAKIT_HOME/env.ikt"
    if [ $? -ne 0 ]; then
      docker run --rm -v $BOOTSTRAP_VOLUME:$INFRAKIT_HOME alpine:3.5 sh -c "echo '{{ global \"/aws/stackname\" \"$clusterid\" }}' >> $INFRAKIT_HOME/env.ikt" || exit 1
    fi
    # set a docker engine label if it's not already set. Useful when deploying locally
    docker run --rm -v $BOOTSTRAP_VOLUME:$INFRAKIT_HOME alpine:3.5 sh -c "grep -q /docker/label/cluster/value $INFRAKIT_HOME/env.ikt"
    if [ $? -ne 0 ]; then
      docker run --rm -v $BOOTSTRAP_VOLUME:$INFRAKIT_HOME alpine:3.5 sh -c "echo '{{ global \"/docker/label/cluster/value\" \"$clusterid\" }}' >> $INFRAKIT_HOME/env.ikt" || exit 1
      echo "$clusterid"
    fi
    # set the provisioner IP if not already set. Useful when bootstraping with the first manager
    docker run --rm -v $BOOTSTRAP_VOLUME:$INFRAKIT_HOME alpine:3.5 sh -c "grep -q /bootstrap/ip $INFRAKIT_HOME/env.ikt"
    if [ $? -ne 0 ]; then
      local _ip
      _ip=$(_get_ip)
      docker run --rm -v $BOOTSTRAP_VOLUME:$INFRAKIT_HOME alpine:3.5 sh -c "echo '{{ global \"/bootstrap/ip\" \"$_ip\" }}' >> $INFRAKIT_HOME/env.ikt" || exit 1
    fi
    if [ "x$provider" = "xdocker" ]; then
        echo "making sure the network $BRIDGE_NETWORK is created" >&2
        docker network create -d bridge $BRIDGE_NETWORK  2>/dev/null
        INFRAKIT_OPTIONS="$INFRAKIT_OPTIONS --network $BRIDGE_NETWORK"
    fi

    _infrakit_image=$(_get_infrakit_image $provider) || exit 1

    echo "starting up InfraKit (image $_infrakit_image)" >&2
    docker run -d --restart always --name infrakit \
         -v $CERT_DIR:/etc/docker \
         $INFRAKIT_OPTIONS $INFRAKIT_PLUGINS_OPTIONS -e PLUGIN_DIR=$INFRAKIT_HOME/${provider} $_infrakit_image \
         infrakit plugin start --wait --config-url $CONTAINER_PLUGINS_CFG --exec os --log 5 \
         manager group-stateless flavor-swarm flavor-vanilla flavor-combo $_extra_plugins >&2
    _should_wait_for_plugins=1
  else
    echo "infraKit container is already started" >&2
  fi
  return $_should_wait_for_plugins
}

# run an infrakit plugin as a container
# return 1 if a new plugin has been started
_run_ikt_plugin_container() {
  local _should_wait_for_plugins=0
  local _plugin
  local _image
  for _plugin in $@; do
    echo $MANAGED_PLUGINS | grep -qw $_plugin
    if [ $? -eq 0 ]; then
      # already managed by the main infrakit, no need to run it separately
      continue
    fi
    echo "$NON_CONTAINERIZED_PLUGINS" | grep -qw $_plugin
    if [ $? -ne 0 ]; then
      docker container ls --format '{{.Names}}' | grep -qw instance-plugin-$_plugin
      if [ $? -ne 0 ]; then
        # first, cleanup the pid and socket files
        # TODO: container way
        rm -f $INFRAKIT_HOME/plugins/instance-${_plugin}*
        _image=$(eval echo \${INFRAKIT_$(echo $_plugin | tr '[:lower:]' '[:upper:]')_IMAGE})
        if [ -z "$_image" ]; then
            echo "no image defined for plugin $_plugin" >&2
            exit 1
        fi
        if [ "$_plugin" = "docker" ]; then
            INFRAKIT_OPTIONS="$INFRAKIT_OPTIONS --network $BRIDGE_NETWORK"
        fi
        echo "starting up InfraKit $_plugin plugin (image $_image)..." >&2
        docker run -d --restart always --name instance-plugin-$_plugin \
             $INFRAKIT_OPTIONS $INFRAKIT_PLUGINS_OPTIONS $_image \
             infrakit-instance-$_plugin --log 5 >&2
        if [ $? -ne 0 ]; then
            echo "unable to start the $_plugin plugin" >&2
            exit 1
        fi
        _should_wait_for_plugins=1
      else
        echo "$_plugin container is already running" >&2
      fi
    fi
  done
  return $_should_wait_for_plugins
}

# destroy the instances managed by infrakit
_destroy_groups() {
  local _groups
  local _group
  _groups=$(docker exec infrakit infrakit group ls 2>/dev/null | tail -n +2)
  for _group in $_groups; do
    docker exec infrakit grep -q "\"$_group\"" $INFRAKIT_HOME/config.json
    if [ $? -eq 0 ]; then
      docker exec infrakit infrakit group destroy $_group
    fi
  done
}

# kill the infrakit container
_kill_ikt() {
  docker container rm -f infrakit >/dev/null 2>&1 || killall infrakit >/dev/null 2>&1 && echo "infrakit has been destroyed" >&2
}

# kill the registry container (but keep the volume)
_kill_registry(){
  docker container rm -f registry >/dev/null 2>&1 && echo "registry has been destroyed" >&2
}

# kill the infrakit plugins (container or process)
_kill_plugins() {
  local _plugin
  for _plugin in $VALID_PROVIDERS; do
    docker container rm -f instance-plugin-$_plugin >/dev/null 2>&1 || killall infrakit-instance-$_plugin >/dev/null 2>&1 && echo "$_plugin plugin has been destroyed" >&2
  done
}

# removes the configuration files
_clean_config() {
  if [ -d $CERT_DIR ]; then
    rm -f $CERT_DIR/ca.pem $CERT_DIR/client.pem $CERT_DIR/client-key.pem
    echo "client certificates have been removed" >&2
  fi
  docker volume inspect $BOOTSTRAP_VOLUME >/dev/null 2>&1
  if [ $? -eq 0 ]; then
    docker volume rm $BOOTSTRAP_VOLUME >/dev/null 2>&1 && echo "configuration volume deleted" >&2
  else
    echo "no configuration volume to delete" >&2
  fi
}

# convert the template of the configuration file
_prepare_config_container() {
  echo "prepare the InfraKit configuration file..." >&2
  local _config=$(mktemp)
  docker exec infrakit infrakit template --log 5 --url $CONTAINER_CONFIG_TPL > $_config
  docker cp $_config infrakit:$INFRAKIT_HOME/config.json || exit 1
  if [ $? -ne 0 ]; then
    echo "failed, template URL was $CONTAINER_CONFIG_TPL" >&2
    rm $_config
    exit 1
  fi
  rm $_config
}

# deploy the infrakit configuration
_deploy_config_container() {
  echo "deploy the configuration..." >&2
  docker exec infrakit infrakit manager commit file://$INFRAKIT_HOME/config.json >&2
}

# run a registry service for Docker image availability in the cluster
_run_registry() {
  docker ps | grep -qw registry || \
  docker run --name=registry --detach --network=$BRIDGE_NETWORK \
             -p=5000:5000 --restart=unless-stopped \
             --volume=registry:/var/lib/registry \
             --label="$CLUSTER_LABEL_NAME=$clusterid" --label="host.role=cluster" \
             registry:2 >&2
}

_status(){
  local _clusterid=$1
  local _groups
  local _group
  local _esize
  local _csize
  local _rc=0
  _groups=$(docker exec infrakit infrakit group ls -q | grep $_clusterid)
  if [ $? -ne 0 ]; then
    echo "no InfraKit group definition found for this cluster" >&2
    return 1
  fi
  for _group in $_groups; do
    # expected size
    _esize=$(docker exec infrakit infrakit metadata cat group-stateless/specs/$_group/Properties/Allocation/Size)
    if [ $? -ne 0 ]; then
      echo "failed to get metadata for group $_group" >&2
      return 1
    fi
    if [ $_esize -eq 0 ]; then
      _esize=$(docker exec infrakit infrakit metadata cat group-stateless/specs/$_group/Properties/Allocation/LogicalIDs | awk -F ',' '{print NF}')
      if [ $? -ne 0 ]; then
        echo "failed to get metadata for group $_group" >&2
        return 1
      fi
    fi
    # current size
    _csize=$(docker exec infrakit infrakit metadata ls group-stateless/groups/$_group/Instances | wc -l)
    if [ $? -ne 0 ]; then
      echo "failed to get metadata for group $_group" >&2
      return 1
    fi
    if [ $_csize -ne $_esize ]; then
      echo "$_group has not converged: expected size = $_esize, current size: $_csize" >&2
      ((++_rc))
    else
      echo "$_group has converged: size = $_esize" >&2
    fi
  done
  if [ $_rc -eq 0 ]; then
    echo "all groups have converged" >&2
  fi
  return $_rc
}

_list(){
  local _clusterid=$1
  local _groups
  local _group
  _groups=$(docker exec infrakit infrakit group ls -q | grep $_clusterid)
  if [ $? -ne 0 ]; then
    echo "no InfraKit group definition found for this cluster" >&2
    return 1
  fi
  for _group in $_groups; do
    docker exec infrakit infrakit instance --name=instance-$provider describe --tags atomiq.clusterid=$_clusterid --tags infrakit.group=$_group -q 2>&1 | awk -v clusterid=$_clusterid -v group=$_group '{print clusterid, group, $2, $1}'
    if [ $? -ne 0 ]; then
      echo "failed to describe group $_group" >&2
      return 1
    fi
  done
}

# where to deploy
VALID_TARGETS="aws local"
# Infrakit plugins used for deployment
VALID_PROVIDERS="aws docker terraform"
# we can't run InfraKit in a container for these plugins
NON_CONTAINERIZED_PLUGINS=""
# providers managed by infrakit (integrated plugin)
MANAGED_PLUGINS="terraform"
target=local
default_target=1
provider=docker
default_provider=1
envfile=""
providerfile=""
manager_count=1
worker_count=2
clean=0
init_swarm=0
swarm_join_ip=""
label=""
force_start=0
status_request=""
list_request=""
while getopts ":1j:t:p:e:m:w:i:l:s:hfd" opt; do
  case $opt in
  1)
      init_swarm=1
      ;;
  j)
      swarm_join_ip=$OPTARG
      ;;
  w)
      worker_count=$OPTARG
      ;;
  m)
      manager_count=$OPTARG
      ;;
  i)
      # clusterid
      label=$OPTARG
      ;;
  t)
      echo "$VALID_TARGETS" | grep -wq "$OPTARG" && target=$OPTARG
      if [ -z "$target" ]; then
          echo "valid targets are $VALID_TARGETS" >&2
          exit 1
      fi
      default_target=0
      ;;
  p)
      # provider[:envfilepath]]
      f1=$(echo "$OPTARG" | cut -f1 -d:)
      f2=$(echo "$OPTARG" | cut -f2 -d:)
      if [ -n "$f2" ] && [ "x$f1" != "x$f2" ]; then
        providerfile="$f2"
        if [ ! -f "$providerfile" ]; then
          echo "Configuration file for $f1 was not found ($providerfile)" >&2
          exit 1
        fi
      fi
      echo "$VALID_PROVIDERS" | grep -wq "$f1" && provider=$f1
      if [ -z "$provider" ]; then
          echo "valid providers are $VALID_PROVIDERS" >&2
          exit 1
      fi
      default_provider=0
      ;;
  l)
      list_request=$OPTARG
      ;;
  s)
      status_request=$OPTARG
      ;;
  h)
      echo "usage: $(basename $0) [-t target] [-p provider] [-m manager_count] [-w worker_count] [-i CLUSTERID] [-l CLUSTERID] [-s CLUSTERID] [--init-swarm] [--join-swarm] [-f] [-h]"
      exit 0
      ;;
  f)
      # force start
      force_start=1
      ;;
  d)
      clean=1
      ;;
  e)
      envfile=$OPTARG
      if [ ! -f "$envfile" ]; then
        echo "$envfile does not exist" >&2
        exit 1
      fi
      ;;
  \?)
      echo "invalid option: -$OPTARG" >&2
      exit 1
      ;;
  :)
      echo "option -$OPTARG requires an argument." >&2
      exit 1
      ;;
  esac
done
shift "$((OPTIND-1))"

# in case only the target has been set, and it's not local, use terraform as default provider
if [ "x$target" != "xlocal" ] && [ $default_provider -eq 1 ]; then
    provider=terraform
fi

if [ -n "$status_request" ]; then
  _status $status_request
  exit $?
fi
if [ -n "$list_request" ]; then
  _list $list_request
  exit $?
fi
if [ $clean -eq 1 ]; then
  _destroy_groups
  _kill_plugins
  _kill_ikt
  _kill_registry
  _clean_config
  exit
fi
if [ $init_swarm -eq 1 ]; then
  _init_swarm || exit 1
fi
if [ -n "$swarm_join_ip" ]; then
  _init_swarm $swarm_join_ip || exit 1
  exit 0
fi
_set_source $1
_set_size
#if [ "$provider" != "docker" ]; then _run_certificate_service; _get_client_certificate; fi
clusterid=$(_set_clusterid)
if [ "$provider" = "docker" ]; then _run_registry; fi
_pull_images $provider
_run_ikt_container $provider
started=$?
_run_ikt_plugin_container $provider
started=$((started + $?))
if [ $started -gt 0 ]; then
  echo -n "waiting for plugins to be available..." >&2
  rc=1
  loop=1
  looplimit=10
  while [ $rc -ne 0 ]; do
    docker exec infrakit infrakit manager inspect 2>&1 | egrep -qe 'leader.*=.*true'
    rc=$?
    docker exec infrakit infrakit plugin ls 2>/dev/null | grep -q "instance-$provider"
    rc=$((rc+$?))
    docker exec infrakit infrakit instance --name=instance-$provider describe >/dev/null 2>&1
    rc=$((rc+$?))
    if [ $((++loop)) -gt $looplimit ]; then
      echo " aborting after $looplimit attempts" >&2
      docker inspect infrakit >&2
      exit 1
    fi
    sleep 1
    echo -n "." >&2
  done
  echo >&2
fi
_prepare_config_container
_deploy_config_container
echo done >&2
